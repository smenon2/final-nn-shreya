{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a73a563-754f-4f60-8007-a2621bf62576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dependencies\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nn import nn, io, preprocess\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda503e8-afb9-4c6b-8071-6e621663a854",
   "metadata": {},
   "source": [
    "I will first read in the data. We then have to make the negative examples the same sequence lengths as the positive examples. All of the positive examples have 17bp so I'll just loop through the negative sequences and create 17bp sub-sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5ee693-ebe2-4904-a4ea-c71548443454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive examples: 137\n",
      "Negative examples: 183296\n"
     ]
    }
   ],
   "source": [
    "positives = io.read_text_file(\"data/rap1-lieb-positives.txt\")\n",
    "negatives = io.read_fasta_file(\"data/yeast-upstream-1k-negative.fa\")\n",
    "\n",
    "\n",
    "# Make the negative sequences the same length as the positive sequences \n",
    "# We see that each of the positive sequences is 17bp \n",
    "\n",
    "seq_length = len(positives[0])\n",
    "negative_trimmed = []\n",
    "for s in negatives:\n",
    "    seq = str()\n",
    "    for i in range(len(s)):\n",
    "        seq = seq + s[i] \n",
    "        \n",
    "        if i > 0 and i % seq_length == 0:\n",
    "            negative_trimmed.append(seq)\n",
    "            seq = str()\n",
    "    \n",
    "# Combine and create labels\n",
    "sequences = positives + negative_trimmed\n",
    "labels = [True] * len(positives) + [False] * len(negative_trimmed)\n",
    "\n",
    "# ensure each example has proper number of base pairs\n",
    "sequences = [s[:seq_length] for s in sequences]\n",
    "\n",
    "# Print counts\n",
    "print(\"Positive examples:\", len(positives))\n",
    "print(\"Negative examples:\", len(negative_trimmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd09a4-60cb-4bc1-9315-e556b07f0ebe",
   "metadata": {},
   "source": [
    "We then see that we have to subsample such that we equal number of examples for both positive and negative sequences. I chose to implement a function that upsamples the smaller class. I think this makes sense here because there are so few positive examples - so if I were to downsample the bigger class, there would be too few examples to train (fewer after splitting between training and validation). This will increase the number of positive examples and overall number of labeled examples available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e3699b-3d13-4e28-8cc1-b0e564a137b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, labels = preprocess.sample_seqs(np.array(sequences), np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a2810-6aef-49aa-b198-360c643543bb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "Now we will one hot encode the sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d62b3564-4fb0-4262-ac44-1517c1e7386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess.one_hot_encode_seqs(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63cda4c-790f-4a47-84f7-41c1e996f026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11e609-2524-4e84-bf98-fa6ea5fa8b0d",
   "metadata": {},
   "source": [
    "Now, we can split the data into training and testing sets. I just use the sklearn train_test_split function to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b1aedc-9084-40a6-bfe9-5cf8de42a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, np.array(labels), test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c818f2f-c5f3-468e-b1a7-9c4bef7cf3a9",
   "metadata": {},
   "source": [
    "Now, I create a tuning grid with different hyperparameter values. I will loop through all of the different combinations, fit a model and then select the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adce4402-a6e1-448e-8fa8-c4ae2496ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "batch_grid = [10, 20, 50, 150, 200, 250, 300, 500, 750, 1000]\n",
    "epochs_grid = [5, 10, 20, 30, 40, 50, 100, 200, 300, 500]\n",
    "\n",
    "comb_array = np.array(np.meshgrid(lr_grid, batch_grid, epochs_grid)).T.reshape(-1, 3)\n",
    "layers = [{\"input_dim\": 68, \"output_dim\": 34, \"activation\": \"sigmoid\"},\n",
    "          {\"input_dim\": 34, \"output_dim\": 17, \"activation\": \"sigmoid\"},\n",
    "          {\"input_dim\": 17, \"output_dim\": 1, \"activation\": \"sigmoid\"},]\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779004ee-972c-44ee-9c8b-52b41172cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_grid = []\n",
    "count = 0\n",
    "for i in comb_array:\n",
    "    count += 1 \n",
    "    lr = i[0]\n",
    "    batch_size = i[1]\n",
    "    epochs = int(i[2])\n",
    "    net = nn.NeuralNetwork(layers, lr = lr, seed = 42, batch_size = batch_size, epochs = epochs, loss_function = \"bce\")\n",
    "    train_losses, val_losses = net.fit(X_train, y_train, X_test, y_test)\n",
    "    # We want to save the test loss for the last epoch\n",
    "    test_loss = val_losses[-1]\n",
    "    tune_grid.append([lr, batch_size, epochs, test_loss])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "d5c83f3b-bf96-4b80-a06f-8836b503734c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 68 into shape (1,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[277], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNeuralNetwork(layers, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m, seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, loss_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Reshape 1D arrays to 2D so the dimensions work\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m      8\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, X_test, y_test)\n",
      "File \u001b[0;32m~/Desktop/BMI203/final-nn-shreya/nn/nn.py:351\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss function not valid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m# Backprop and update parameters\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     grad_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_params(grad_dict)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Find average training loss and append to list\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/BMI203/final-nn-shreya/nn/nn.py:259\u001b[0m, in \u001b[0;36mNeuralNetwork.backprop\u001b[0;34m(self, y, y_hat, cache)\u001b[0m\n\u001b[1;32m    256\u001b[0m Z_curr \u001b[38;5;241m=\u001b[39m cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(layer_idx)]\n\u001b[1;32m    257\u001b[0m A_prev \u001b[38;5;241m=\u001b[39m cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(idx)]\n\u001b[0;32m--> 259\u001b[0m dA_prev, dW_curr, db_curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ_curr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdA_curr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m dA_curr \u001b[38;5;241m=\u001b[39m dA_prev\n\u001b[1;32m    262\u001b[0m grad_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(layer_idx)] \u001b[38;5;241m=\u001b[39m dW_curr\n",
      "File \u001b[0;32m~/Desktop/BMI203/final-nn-shreya/nn/nn.py:216\u001b[0m, in \u001b[0;36mNeuralNetwork._single_backprop\u001b[0;34m(self, W_curr, b_curr, Z_curr, A_prev, dA_curr, activation_curr)\u001b[0m\n\u001b[1;32m    214\u001b[0m dW_curr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(activation_back\u001b[38;5;241m.\u001b[39mT, A_prev)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# NOT SURE ABOUT THIS:\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m db_curr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation_back\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_curr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m dA_prev \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(activation_back, W_curr)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dA_prev, dW_curr, db_curr\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 68 into shape (1,1)"
     ]
    }
   ],
   "source": [
    "net = nn.NeuralNetwork(layers, lr = 0.001, seed = 42, batch_size = 10, epochs = 5, loss_function = \"bce\")\n",
    "train_losses, val_losses = net.fit(X_train, y_train, X_test, y_test)\n",
    "# Plot losses\n",
    "fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n",
    "ax[0].plot(range(20), train_losses)\n",
    "ax[0].set_title(\"Training\")\n",
    "ax[1].plot(range(20), val_losses)\n",
    "ax[1].set_title(\"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b025e-7d5e-4f34-bd7d-ba9205735efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy\n",
    "pred = (net.predict(X_val) >= 0.5).astype(int)\n",
    "print(\"Accuracy on validation set\": {np.sum(pred == y_val) / len(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e51a1-15dd-45a1-ad14-84392570a230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a45d08-ac07-4ba8-88a8-7147a33a91dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291dfc60-982e-48a0-958f-36976ece0613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9562b1e-def6-4e8a-9dec-ce2c69bcdccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "20aa6729-e14a-4302-b8b2-590368402ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "eb6e253f-82bb-4e27-bc42-235f3e56ee87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ecfa8ced-b2d0-43f3-b54e-4391d043475a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545f1c7-477e-493d-9b9a-d7e3f74df6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4baf8b-94bc-45e5-8683-89ee0533f580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2188a-cbad-47f3-a544-a02954b55d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26996d-d48d-45d8-8304-1a84c2eac04c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d01e5f-dcd1-4a7a-bc77-7a026ba2e517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd212d5-f7eb-4f6a-a999-a87693d9de77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef818236-bd14-467d-8dd2-20b5013b09e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5416974-f742-4095-aa37-39907176abdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3930c8-7827-49a4-943a-e43763825b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BMI203]",
   "language": "python",
   "name": "conda-env-BMI203-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
